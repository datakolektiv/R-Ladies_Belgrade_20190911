---
title: Semantic Web Technologies and Wikidata from R, R-Ladies Belgrade Meetup 2019/09/11
author:
- name: Goran S. MilovanoviÄ‡
  affiliation: Wikimedia Deutschland, Data Scientist, DataKolektiv, Owner
abstract: 
output:
  html_notebook:
    code_folding: show
    theme: spacelab
    toc: yes
    toc_float: yes
    toc_depth: 5
  html_document:
    toc: yes
    toc_depth: 5
---

![](img/DK_Logo_100.png) ![](img/Wikimedia_Deutschland_Logo_small.png) ![](img/Wikidata-logo-en.png)

***
### Notebook: Accessing Wikidata and Wikipedia from R
**Feedback** should be send to `goran.milovanovic@gmail.com`. 
This notebook accompanies the [R-Ladies Belgrade Meetup, 2019/09/11](https://www.meetup.com/rladies-belgrade/events/264427122/)
[Startit, Belgrade](https://startit.rs/beograd/)

***

## 0. Setup

**Note.** The following chunk loads the packages and defines the project directory tree.

```{r echo = T, eval = T, message = F}
### --- setup

## - libraries
library(data.table)
library(dplyr)
library(stringr)
library(WikidataQueryServiceR)
library(WikipediR)
library(WikidataR)
library(httr)
library(jsonlite)
library(rvest)

### --- directories
dataDir <- 'data/'
analyticsDir <- 'analytics/'
```


## 1. Accessing Wikidata from R: the API and its R client library for Wikidata

### 1A. The {WikidataR} package

**Note.** If you want to learn how to use Wikidata, you probably first need to study the following page thoroughly: [Wikibase/DataModel/JSON](https://www.mediawiki.org/wiki/Wikibase/DataModel/JSON). Do not confuse Wikidata and Wikibase. While [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) is a data set, [Wikibase](https://wikiba.se/) is what runs that (and not only that) data set.

The {WikidataR} package wraps-up the [Wikidata MediaWiki API](https://www.wikidata.org/wiki/Wikidata:Data_access#MediaWiki_API) (see [API documentation](https://www.wikidata.org/w/api.php)) calls for you. If you are about to use the Wikidata directly API, use the modules that return JSON: `wbgetentities` and `wbsearchentities`. 

**Example.** Retrieve [`Q1860`](https://www.wikidata.org/wiki/Q1860) (it is: **English**, in the sense of: English language) and study its structure.

```{r echo = T, eval = T}
# - Retrieve the Wikidata item: English (Q1860) 
item <- get_item(id = 'Q1860')
class(item)
```

It is really a list:

```{r echo = T, eval = T}
# - Retrieve the Wikidata item: English (Q1860) 
length(item)
```

Hm?

```{r echo = T, eval = T}
print(paste0("length(item[[1]]): ", length(item[[1]])))
item_components <- sapply(item, names)
item_components
```

`$type` is `item`:

```{r echo = T, eval = T}
# - item is English (Q1860) 
item[[1]]$type
```

Labels is a list of `labels` in all available languages:

```{r echo = T, eval = T}
labels <- lapply(item[[1]]$labels, function(x) {
  d <- unlist(x)
  data.frame(language = d[1],
             value = d[2], 
             stringsAsFactors = F)
})
labels <- rbindlist(labels)
head(labels, 10)
```

Why so complicated? Well, because Wikidata is a data structure of immense complexity. I know it doesn't help.
Let's take a look at the structure of out `item` in more detail:

```{r echo = T, eval = T}
labs <- item[[1]]$labels
class(labs)
```

A list again. Now:

```{r echo = T, eval = T}
head(labs, 3)
```

```{r echo = T, eval = T}
lab <- labs[[1]]
class(lab)
print(lab)
```

```{r echo = T, eval = T}
unlist(lab)
```

Item descriptions, in all available languages:

```{r echo = T, eval = T}
descriptions <- lapply(item[[1]]$descriptions, function(x) {
  d <- unlist(x)
  data.frame(language = d[1],
             value = d[2], 
             stringsAsFactors = F)
})
descriptions <- rbindlist(descriptions)
head(descriptions, 10)
```

**Note.** Try using `sapply()` in place of `lapply()` and study what happens. 

Item aliases, in all available languages:

```{r echo = T, eval = T}
aliases <- lapply(item[[1]]$aliases, function(x) {
  d <- unlist(x)
  data.frame(language = d[1],
             value = d[2], 
             stringsAsFactors = F)
})
aliases <- rbindlist(aliases)
head(aliases, 10)
```

Sitelinks (the titles of the respective Wiki pages) in all available Wikimedia projects:

```{r echo = T, eval = T}
sitelinks <- lapply(item[[1]]$sitelinks, function(x) {
  d <- unlist(x)
  data.frame(project = d[1],
             value = d[2], 
             stringsAsFactors = F)
})
sitelinks <- rbindlist(sitelinks)
head(sitelinks, 10)
```

The main course: **claims**

```{r echo = T, eval = T}
# - list of all claims for English (Q1860) 
claims <- names(item[[1]]$claims)
class(claims)
head(claims, 20)
```

What is `P4132`? Use `WikidataR::get_property()`:

```{r echo = T, eval = T}
prop <- get_property(id = 'P4132')
prop[[1]]$labels$en$value
```

Let's describe English language by Wikidata classes to which it belongs.
**Q.** Of which Wikidata classes is `English (Q1860)` an [`instance of (P31)`](https://www.wikidata.org/wiki/Property:P31)?

We first need to study the structure of Wikidata claims.

```{r echo = T, eval = T}
claims <- item[[1]]$claims
class(claims)
```

```{r echo = T, eval = T}
claims[[1]]
class(claims)
```
```{r echo = T, eval = T}
claims[[1]]$mainsnak
```

Ooops - do we have a nested `data.frame` here?

```{r echo = T, eval = T}
str(claims[[1]]$mainsnak)
```

Oh yes we do. We do not like nested data frames in R and should do anything to avoid them. 
Now, `jsonlite::flatten()` does the job:

```{r echo = T, eval = T}
flattenedClaim <- jsonlite::flatten(claims[[1]]$mainsnak)
str(flattenedClaim)
```

Please mind the new `colnames()`:

```{r echo = T, eval = T}
colnames(flattenedClaim)
```

and yes, `.` is a valid character in an R `data.frame` column name.

```{r echo = T, eval = T}
flattenedClaim$datavalue.value
```

Now, what does this claim tell us about English?

```{r echo = T, eval = T}
t(flattenedClaim)
```

The `snaktype` field describes that this piece of data is some `value`. Nevermind the `hash` filed. The property field carries the Wikidata property that this statement uses as its predicate in a triple: `Subject (English)`-`P2924`-`1821310`, where the value of `1821310` is found in the `datavalue.value` field and is of type `string` (described by the `datavalue.type` field). What is `P2924`?

```{r echo = T, eval = T}
claimProperty <- get_property('P2924')
claimProperty
```

So, we have learned that `1821310` is the ID of the entry for the English language in the Great Russian Encyclopedia, and we have also learned that Wikidata uses the [`P2924`](https://www.wikidata.org/wiki/Property:P2924) property as an external identifier to point to this resource! 

There are many more complex claims than this one, however. How many people speak English in the World?
The relevant Wikidata property for this is [`P1098 number of speakers`](https://www.wikidata.org/wiki/Property:P1098).

```{r echo = T, eval = T}
numSpeakers <- claims[[which(names(claims) == "P1098")]]
str(numSpeakers)
```

Wow. Ok, let's dive into it:

```{r echo = T, eval = T}
numSpeakers <- jsonlite::flatten(numSpeakers, recursive = T)
str(numSpeakers)
```

Oh no.

```{r echo = T, eval = T}
colnames(numSpeakers)
```

First extract the statement ranks:

```{r echo = T, eval = T}
ranks <- numSpeakers$rank
ranks
```

We now that we have found four (4) statements on `P1098 number of speakers` for English.
What is this: `qualifiers-order`?

```{r echo = T, eval = T}
numSpeakers$`qualifiers-order`
```

Now we now that each statement for `P1098 number of speakers` for English always has two additional qualifiers: [`P585 point in time`](https://www.wikidata.org/wiki/Property:P585) and [`P518 applies to part`](https://www.wikidata.org/wiki/Property:P518). We need to find their values too.

```{r echo = T, eval = T}
colnames(numSpeakers)
```

First, what data are stated as candidate number of speakers measure for English? 

```{r echo = T, eval = T}
numSpeakers$mainsnak.datavalue.value.amount
numSpeakers$mainsnak.datavalue.value.unit
data = numSpeakers$mainsnak.datavalue.value.amount
```

Ok. Now, how are these four number different? The qualifiers carry that information:

```{r echo = T, eval = T}
class(numSpeakers$qualifiers.P585)
str(numSpeakers$qualifiers.P585)
qualifier_P585 <- sapply(numSpeakers$qualifiers.P585, 
                         function(x) {
                           return(x$datavalue$value$time)
                         })
qualifier_P585
```

Ok, now the same for `P518 applies to part`:

```{r echo = T, eval = T}
str(numSpeakers$qualifiers.P518)
```

```{r echo = T, eval = T}
qualifier_P518 <- sapply(numSpeakers$qualifiers.P518, 
                         function(x) {
                           return(x$datavalue$value$id)
                         })
qualifier_P518
```

We have the full information on the number of speakers of English from Wikidata now:

```{r echo = T, eval = T}
nspeakers <- data.frame(P518 = qualifier_P518, 
                        P585 = qualifier_P585, 
                        data = data,
                        rank = ranks,
                        stringsAsFactors = F)
colnames(nspeakers) <- c('applies to part (518)', 
                         'point in time (585)', 
                         'data',
                         'rank')
itemValues <- get_item(nspeakers$`applies to part (518)`)
nspeakers$`applies to part (518)` <- sapply(itemValues, function(x) {x$labels$en$value})
nspeakers
```

It is not over yet:

```{r echo = T, eval = T}
nspeakers$data <- as.numeric(nspeakers$data)
nspeakers
```

And now the time:

```{r echo = T, eval = T}
nspeakers$`point in time (585)` <- substr(nspeakers$`point in time (585)`, 2, 5)
nspeakers
```

Finally: English has `379,007,140` speakers as of 2019.

### 1B. The Wikidata MediaWiki API

You really need to browse the documentation for this carefully.
Focus on the following modules: `wbgetentities` and `wbsearchentities`.

*Example.* `wbgetentities`

```{r echo = T, eval = T}
# - Wikidata MediaWiki API prefix
APIprefix <- 'https://www.wikidata.org/w/api.php?action=wbgetentities&'
# - Random Wikidata item
ids <- paste0("Q", round(runif(20, 1, 1000)))
ids <- paste0(ids, collapse = "|")
# - Compose query
query <- paste0(APIprefix, 
                    'ids=', ids, '&',
                    'props=labels&languages=en&sitefilter=wikidatawiki&format=json')
# - contact the API
result <- GET(url = URLencode(query))
# - raw to char
result <- rawToChar(result$content)
# - to JSON:    
result <- fromJSON(result)
# - parse JSON:
itemLabels <- unlist(lapply(result$entities, function(x) {
  x$labels$en$value
  }))
itemLabels <- data.frame(entity_id = names(itemLabels),
                         label = itemLabels,
                         stringsAsFactors = F)
itemLabels
```

*Example.* `wbsearchentities`

```{r echo = T, eval = T}
# - Wikidata MediaWiki API prefix
APIprefix <- 'https://www.wikidata.org/w/api.php?action=wbsearchentities&'
# - search query
searchQuery <- "functional programming"
# - Compose query
query <- paste0(APIprefix, 
                    'search=', searchQuery, '&',
                    'language=en&strictlanguage=true&format=json')
# - contact the API
result <- GET(url = URLencode(query))
# - raw to char
result <- rawToChar(result$content)
# - to JSON:    
searchResult <- fromJSON(result, simplifyDataFrame = T)
# - fetch labels and descriptions
searchResult <- get_item(searchResult$search$id)
# - labels and descriptions
descriptions <- sapply(searchResult, function(x) {
  paste0(x$labels$en$value, ": ", x$descriptions$en$value)
})
descriptions
```

## 2. Accessing Wikidata from R: SPARQL

[SPARQL](https://en.wikipedia.org/wiki/SPARQL) is a language on its own:

"SPARQL (pronounced \"sparkle\", a recursive acronym for SPARQL Protocol and RDF Query Language) is an RDF query languageâ€”that is, a semantic query language for databasesâ€”able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. It was made a standard by the RDF Data Access Working Group (DAWG) of the World Wide Web Consortium, and is recognized as one of the key technologies of the semantic web.[citation needed] On 15 January 2008, SPARQL 1.0 became an official W3C Recommendation, and SPARQL 1.1 in March, 2013." 
Source: [Wikipedia](https://en.wikipedia.org/wiki/SPARQL), retrieved on: `2019/09/09`.

Wikidata maintains a very nice [SPARQL tutorial](https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial) and also provides tons of [query examples](https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples).

### 2A. SPARQL via WDQS (Wikidata Query Service)

Please take a look at the following page: [A gentle introduction to the Wikidata Query Service](https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/A_gentle_introduction_to_the_Wikidata_Query_Service).

```{r echo = T, eval = T}
# - NOTE. The Wikidata SPARQL Tutorial: https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial
# - WDQS endPoint:
endPointURL <- "https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query="
# - query:
# - NOTE. For the SELECT wikibase:label "magic", see:
# - https://en.wikibooks.org/wiki/SPARQL/SERVICE_-_Label
query <- 'SELECT ?item ?itemLabel WHERE {
  ?item wdt:P31 wd:Q9143 .
   SERVICE wikibase:label { bd:serviceParam wikibase:language "en". }
}'
res <- GET(url = paste0(endPointURL, URLencode(query)))
res$status_code
```

```{r echo = T, eval = T}
# - decode:
res <- rawToChar(res$content)
substr(res, 1, 2000)
```

```{r echo = T, eval = T}
# - from JSON:
res <- fromJSON(res)
class(res)
```

```{r echo = T, eval = T}
items <- res$results$bindings$item
labels <- res$results$bindings$itemLabel
```

```{r echo = T, eval = T}
# - inspect result:
head(items)
```

```{r echo = T, eval = T}
# - inspect result:
head(labels)
```

```{r echo = T, eval = T}
# - data.frame:
programmingLanguages <- data.frame(items = gsub("http://www.wikidata.org/entity/", "", items$value), 
                                   labels = labels$value, 
                                   stringsAsFactors = F)
head(programmingLanguages, 10)
```

## 3. Accessing Wikidata from R: parsing the Wikidata Dump

First you probably want to take a look at [Wikidata:Database_download](https://www.wikidata.org/wiki/Wikidata:Database_download):

> There are several different kinds of data dumps available. Note that while JSON and RDF dumps are considered stable interfaces, XML dumps are not. Changes to the data formats used by stable interfaces are subject to the [Stable Interface Policy](https://www.wikidata.org/wiki/Wikidata:Stable_Interface_Policy).

To put it in a nutshell: Wikidata dumps are simply copies of *all* data available in Wikidata at some point in time. The fact that there are different dumps reflects only the fact that there are different file formats which can describe what is in a database. The recommended dump format to use for Wikidata is [JSON](https://en.wikipedia.org/wiki/JSON), while you can also rely on the [XML](https://en.wikipedia.org/wiki/XML) or [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework) dumps if you wish.

The existence of the dumps means that there is a beatiful thing that you can do: namely, you can extract *all* data for a particular set of items that you can *arbitrarily* define. For example, you might need to extract all items with geo-coordinates (and there are millions of them in Wikidata) alongisde their IDs and English or Serbian labels. You might need to extract all items that have the property [sex or gender (P21)](https://www.wikidata.org/wiki/Property:P21), the value of that property, the profession of the respective individual, or any other data on those individuals that you might be interested in. In other words, when you need tons of data from Wikidata, and have a specified information schema that you want to access and use in your future analyses or projects, the dumps are the way to go. It is not efficient (and might be impossible as well) to go for SPARQL via [WDQS](https://query.wikidata.org/) or [Mediawiki API]()https://www.wikidata.org/w/api.php for heavy tasks like these. 

In the following section we will focus on the [JSON](https://en.wikipedia.org/wiki/JSON) database dump. The JSON dump lives here:

[https://dumps.wikimedia.org/wikidatawiki/entities/](https://dumps.wikimedia.org/wikidatawiki/entities/)

and typically the file that you are looking for is the latest dump: `latest-all.json.bz2`.

The `.bz2` extension means that the file is compressed with [bzip2](https://en.wikipedia.org/wiki/Bzip2). Don't worry, the `bzip2` compression can be read from base R: you will not need to decompress the whole dump file to be able to process it.

Once again: in order to learn *how* to extract *exactly the data that you need* you will need to study the Wikidata JSON data model (from: [Wikibase/DataModel/JSON](https://www.mediawiki.org/wiki/Wikibase/DataModel/JSON)) and probably experiment on several items first from [WikidataR](https://cran.r-project.org/web/packages/WikidataR/index.html). Your experimentation should result in a clear understanding of what properties, qualifiers, labels, etc. exactly do you need to extract, as well as what the shape of the data set will be - in order to be able to efficiently map the Wikidata JSON structure onto the respective structure in R. While it might sound complicated in the beginning, it turns out be quite feasible in practice. 

**NOTE.** R, as well as Python, is an interpreted programming language, and thus - for reasons of processing efficiency - not the best choice for a task like this. If you know Java, you might wish to take a look at the [Wikidata Toolkit (WDTK)](https://www.mediawiki.org/wiki/Wikidata_Toolkit) library As far as my knowledge goes, the WDTK framework is the fastest library available to process the Wikidata dumps. However, I will show you how you can have the newest Wikidata dump processed in R in much less than a day, a time frame acceptable for a majority of projects that would rely on a semantic knowledge base like Wikidata. Moreover, you can do this on your laptop.  

### 3a. Parsing the JSON Dump w. {rjson}

**NOTE.** For Python people, here is a [similar approach from Aliakbar Akbaritabar (Ali)](https://akbaritabar.netlify.com/how_to_use_a_wikidata_dump). Ali was interested to learn if there is an efficient way to do it in R and then we got in touch before I developed this (essentialy simple, I think) approach. Many thanks to Ali for the inspiration!

Once again, the location of the Wikidata JSON dump:

[https://dumps.wikimedia.org/wikidatawiki/entities/](https://dumps.wikimedia.org/wikidatawiki/entities/)
and the file we need is the one of the latest dump: `latest-all.json.bz2`.

Now, the [structure of the JSON dump](https://www.wikidata.org/wiki/Wikidata:Database_download/en#JSON_dumps_(recommended)):

> JSON dumps containing all Wikidata entities in a single JSON array can be found under https://dumps.wikimedia.org/wikidatawiki/entities/. The entities in the array are not necessarily in any particular order, e.g., Q2 doesn't necessarily follow Q1. The dumps are being created on a weekly basis. [...]
Hint: Each entity object (data item or property) is placed on a separate line in the JSON file, so the file can be read line by line, and each line can be decoded separately as an individual JSON object.

Got it? The idea is: one line in the `latest-all.json.bz2` compressed file represents one Wikidata entity (item or property, nevermind the lexems at this point) and carries all associated data. So the outline of the dump processing approach would be:

1. Download the compressed Wikidata JSON dump from [https://dumps.wikimedia.org/wikidatawiki/entities/](https://dumps.wikimedia.org/wikidatawiki/entities/) and note the directory on your local machine where it resides;

2a. Open a connection to the `latest-all.json.bz2` dump file with the R `bzfile()` function;

2b. Open a connection to some output file, somewhere where you want to store the processed data;

3. Iterate across the dump, line by line, with `readLines()`: a `repeat` loop working until the actual line read is empty would server us good;

4. Use `fromJSON()` from the `{rsjon}` package to parse one JSON array (i.e. one line from the compressed dump file);

5. Since `fromJSON()` will return an R `list`, find the component(s) that you are looking for (reminder: you should have studied the JSON data model first!);

6. Use `writeLines()` to write the processed data as one line, with comma separated fields, so that your output file becomes a nicely readable `.csv` in the end;

7. Close the file connections!

In the following demonstration I will extract all items with English labels from the Wikidata JSON dump. 

**Note 1.** On an Intel i7 (not the newest generation) + 32Gb or RAM this took approximately 10 hours.

**Note 2.** Forget about 32Gb or RAM info: this can be done on an average nowdays available laptop. The memory consumption is really low. I am now considering a parallel execution approach that would significantly lower the total job time.

**Note 3.** *"I need to process several different data models from the Wikidata dump, it would take forever?"* Well, consider writing several R scripts and putting them on crontab... As I have already mentioned, your laptop can do it. Mind the number of cores/threads and how much RAM you need to spend: experimenting with these parameters is essential. But it would do. Yes, with your laptop.

**Note 4.** In my experiments, the popular [{jsonlite}](https://cran.r-project.org/web/packages/jsonlite/index.html) package to parse JSON from R was beaten by an order of magnitude in terms of processing speed by [{rjson}](https://cran.r-project.org/web/packages/rjson/index.html) (published: `2018-06-08`, version 0.2.20).

```{r echo = T, eval = F}
# - setup
library(data.table)
library(rjson)
dumpDir <- '/home/somewhere/yourDumpDirectory/'
outDir <- '/home/somewhere/yourOutputDirectory'
dumpFile <- 'latest-all.json.bz2'
outFile <- 'enlabels_processed.csv'

# - to dump directory
setwd(dumpDir)

# - extract English labels
# - open connection to dump
con <- bzfile(description = dumpFile,
              open = "r", 
              encoding = getOption("encoding"), 
              compression = 1)
# - open connection to outFile
conOut <- file(paste0(outDir, outFile), 
               open = "w", 
               encoding = getOption("encoding"))

# - read initial line: "[" the beginning of the JSON array
f <- readLines(con = con, 
               n = 1, 
               ok = FALSE, 
               warn = TRUE,
               encoding = "unknown", 
               skipNul = FALSE)
# - initiate counter
c <- 0
# - initiate timing
t1 <- Sys.time()
repeat {
  # - counter
  c <- c + 1
  # - read one line from the dump
  f <- readLines(con = con,
                 n = 1,
                 ok = FALSE,
                 warn = TRUE,
                 encoding = "unknown",
                 skipNul = FALSE)
  # - if the line is empty: break (EOF)
  if (length(f) == 0) {
    break
    # - else: parse JSON
  } else {
    # - parse w. rjson::fromJSON, remove "," at the end of the line; 
    # - defensive:
    fjson <- tryCatch({
      rjson::fromJSON(gsub(",$", "", f), 
                      method = "C", 
                      unexpected.escape = "skip",
                      simplify = FALSE)
      }, 
      error = function(condition) {
        FALSE
      })
    # - check if the JSON was parsed correctly
    if (class(fjson) == "logical") {
      next
    }
    # - if fjson$labels$en$value is not null: process and write data
    if (!is.null(fjson$labels$en$value)) {
      writeLines(paste0('"', fjson$id, '"', ",", '"',fjson$labels$en$value, '"'), conOut)
    }
  }
  # - conditional report on 100,000 lines
  if (c %% 100000 == 0) {
    # - report
    print(paste0("### --------------------------- PING!: 100,000:  ", c))
    print(paste0("This took: ", difftime(Sys.time(), t1, units = "mins"), " minutes."))
  }
}
# - close file connections
close(con)
close(conOut)

# - final reporting
print("-------------------------------------------------------")
print(paste0("Job started: ", as.character(t1)))
print(paste0("Total job time: ", difftime(Sys.time(), t1, units = "mins"), " minutes."))
print(paste0("Total job time: ", difftime(Sys.time(), t1, units = "hours"), " hours."))
print("-------------------------------------------------------")

# - check
setwd(outDir)
dataSet <- fread(outFile, 
                 header = F)
```


***
Goran S. MilovanoviÄ‡

DataKolektiv, 2019.

contact: goran.s.milovanovic@gmail.com

![](img/DK_Logo_100.png)


